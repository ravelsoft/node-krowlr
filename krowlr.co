{ DefaultHandler, Parser} = require \htmlparser
events = require \events
fs = require \fs
path = require \path
request = require \request
soupselect = require \soupselect
url = require \url
util = require \util
winston = require \winston

basic_cache = require \./lib/basic_cache

select = soupselect.select
handler = new DefaultHandler
parser = new Parser handler

APPLICATION_NAME = 'krowlr'

simplify = (_url) ->
    o = url.parse _url
    return o.protocol + '//' + o.host + o.path


class PageReference
    counter = 0
    ->
        counter := counter + 1
        @id = counter
        @links = []
        @tries = 1
        @title = ''
        @url = ''

    add_link: (ref) ->
        if ref not of @links and @id !== ref.id
            @links.push ref


class Crawler extends events.EventEmitter
    ->
        super ...

        @max_depth =  5
        @depth_outside_domains = 0
        @start_urls = []
        @accepted_domains = []
        @max_retries = 3
        @retry_wait = 1000
        @processed = {}
        @processed_outside = {}
        @user_agent = 'SCHMÜTZÏ'
        @to_download = 0
        @use_cache = true
        @cache = new basic_cache.BasicCache!
        @rules = []

        @logger = new winston.Logger {
            transports: [ new (winston.transports.Console) {
                level: 'info'
                colorize: 'true'
            }]
        }
    
        @accepted_extensions = ['html', 'htm', 'php']
        @accepted_mime_type = ['text/html']

        @info = @logger.info
        @error = @logger.error
        @warn = @logger.warn

        @on_http_error = null

    log_level: function (level)
        @logger.transports.console.level = level

    add_url: function (_url)
        @start_urls.push _url
        o = url.parse _url
        if o.host not of @accepted_domains
            @accepted_domains.push o.host
            @to_download += 1
        return

    add_urls: function (url_list)
        for u of url_list
            @add_url u
        return

    process_rule: function (_url)
        o = url.parse _url
        result = true
        for rule of @rules
            if o.host.match(rule[0])
                if _url.match(rule[1])
                    result = result and true
                else
                    result = result and false
        return result
            
    accept_mimetype: function (mimetype)
        for type of @accepted_mimetypes
            if mimetype.indexOf type >= 0
                return true
        return false


    fetch_download: function (_url, depth,parent_url, cachefname = '')
        @info "*** Fetching #{_url} #{@to_download}"
        (err, response, body) <~ request _url
        if err
            if @processed[_url].tries < @max_retries
                @processed[_url].tries += 1
                setTimeout (~> @fetch _url, depth), @retry_wait
                return
            else
                @to_download -= 1
                @warn "*** Max retries reached #{_url}"
                return

        if response.statusCode != 200 and @on_http_error
            @on_http_error _url, response, parent_url
            @to_download -= 1
            return

        if @accept_mimetype response.headers['content-type']
            if @use_cache
                @cache.save _url, body

            if response.statusCode != 200 and @on_http_error
                @on_http_error _url, response, parent_url

            @to_download -= 1
            @parse body, _url, depth + 1

    fetch_cache: function (_url, depth, parent_url) 
        @cache.get _url, (err, data) ~>
            if err
                @fetch_download _url, depth, parent_url
            else
                @to_download -= 1
                @parse data, _url, depth + 1

    fetch: function (_url, depth = 0, parent_url = null)
        if @use_cache
            @fetch_cache _url, depth, parent_url
        else
            @fetch_download _url, depth, parent_url

    parse: function (body, base_url, depth)
        parser.parseComplete body
        @info "*** Parsing #{base_url} #{@to_download}"
        base_host = url.parse(base_url).host
        query = select handler.dom, 'title'
        query.forEach (elt) ~>
            if elt.hasOwnProperty 'children'
                @processed[base_url].title = elt.children[0].raw
            else
                o = url.parse base_url
                @processed[base_url].title = o.pathname
   
        m = select handler.dom, 'a'
        m.forEach (elt) ~>
           if elt.hasOwnProperty 'attribs'
                if elt.attribs.hasOwnProperty 'href' and elt.attribs.href?
                    newurl = url.resolve base_url, elt.attribs.href
                    newurl = newurl.replace /&amp;/g, '&'
                    newurl = simplify newurl
                    o = url.parse newurl
                    if o.host of @accepted_domains
                        if not @processed.hasOwnProperty newurl
                            if depth > @max_depth
                                return

                            if 

                            if @rules == [] or @process_rule newurl
                                ref = new PageReference!
                                ref.url = newurl
                                @processed[newurl] = ref
                                @to_download += 1
                                @fetch newurl, depth, base_url
                        else
                            ref = @processed[newurl]
                        @processed[base_url].add_link ref
                    else
                        if @depth_outside_domains > 0
                            if not @processed_outside.hasOwnProperty newurl
                                if base_host of @accepted_domains
                                    newdepth = 0
                                else 
                                    newdepth = depth
                                if depth <= @depth_outside_domains
                                    ref = new PageReference!
                                    ref.url = newurl
                                    @processed_outside[newurl] = ref
                                    @to_download += 1
                                    @fetch newurl, newdepth, base_url
                                
                        
        if @to_download === 0
            @info '*** Ending crawling'
            @emit 'exit'


    start: function ()
        if @use_cache
            <~ @cache.init
            @launch!
        else
            @launch!

    launch: function()
        @info '*** Starting crawling'
        for _url of @start_urls
            _url = simplify _url
            ref = new PageReference!
            ref.url = _url
            @processed[_url] = ref
            @fetch _url

exports.Crawler = Crawler
